{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems are among the most popular applications of data science today. Our target in Recommendation process is to predict the \"rating\" or \"preference\" that a user would give to an item. There are many tech-companies that have already applied them in practice use.\n",
    "* Amazon uses it to suggest products to customers\n",
    "* YouTube uses it to decide which video to play next on autoplay\n",
    "* Facebook uses it to recommend pages to like and people to follow\n",
    "* ...\n",
    "\n",
    "Generally speaking, Recommender System can be classified into 3 types:\n",
    "* Popularity-based recommender\n",
    "* Content-based recommender\n",
    "* Collaborative filtering (CF) recommender\n",
    "    * model-based\n",
    "    * memory-based\n",
    "\n",
    "In this project, we will focus on the last two recommenders. For content-based and memory-based CF, we will build on our own. For model-based CF, we are going to use Python package [`surprise`](https://surprise.readthedocs.io/en/stable/getting_started.html#). Please install `surprise` before using it. \n",
    "\n",
    "Now let's import some packages and read the data we have processed before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# skip all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>Cajun/Creole, Seafood, Steakhouses, Restaurants</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-08 16:39:25</td>\n",
       "      <td>0</td>\n",
       "      <td>tTD3wegtsiHOjVTLgsb7FA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>On yelp 5 stars = Woohoo! as good as it gets! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>joe4i-lcCFd2wViA2agGKw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>Cajun/Creole, Seafood, Steakhouses, Restaurants</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-19 23:47:56</td>\n",
       "      <td>0</td>\n",
       "      <td>aie6cd-6-nbQ5SOMgfSteg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent food and service!! Everyone enjoyed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>CW2rG0leS6vuVHLyvUx4Cg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>Cajun/Creole, Seafood, Steakhouses, Restaurants</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-05 00:49:01</td>\n",
       "      <td>0</td>\n",
       "      <td>HGegJEZMSilsyt7RNcxwCw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We had a great experience!  Super friendly and...</td>\n",
       "      <td>0</td>\n",
       "      <td>hOQ70lGgr6hYRMcoS330Kw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "2  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "\n",
       "                                        categories  avg_stars  cool  \\\n",
       "0  Cajun/Creole, Seafood, Steakhouses, Restaurants        4.0     0   \n",
       "1  Cajun/Creole, Seafood, Steakhouses, Restaurants        4.0     0   \n",
       "2  Cajun/Creole, Seafood, Steakhouses, Restaurants        4.0     0   \n",
       "\n",
       "                  date  funny               review_id  stars  \\\n",
       "0  2018-05-08 16:39:25      0  tTD3wegtsiHOjVTLgsb7FA    5.0   \n",
       "1  2018-06-19 23:47:56      0  aie6cd-6-nbQ5SOMgfSteg    5.0   \n",
       "2  2018-04-05 00:49:01      0  HGegJEZMSilsyt7RNcxwCw    5.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  On yelp 5 stars = Woohoo! as good as it gets! ...       0   \n",
       "1  Excellent food and service!! Everyone enjoyed ...       0   \n",
       "2  We had a great experience!  Super friendly and...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  joe4i-lcCFd2wViA2agGKw  \n",
       "1  CW2rG0leS6vuVHLyvUx4Cg  \n",
       "2  hOQ70lGgr6hYRMcoS330Kw  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../data/last_1_years_restaurant_reviews.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188988, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a content-based recommender system, which is based on features of the item. Here, the item is Restaurant (`business_id`) and features include average rating, cool, useful and text information. We will group by business_id to compute those score-related features. For text info, we use simple NLP techniques to generate word-like features. This will give you a high dimension feature space. Therefore, you should also need to perform Dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we generate some useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by business_id, then average numerical features\n",
    "df_average = df.groupby(['business_id']).mean()\n",
    "\n",
    "# group by business_id, extract categories data\n",
    "categories_series = df.groupby(['business_id']).categories.apply(np.unique)\n",
    "\n",
    "# convert categories data to string remove `[]`\n",
    "categories_series = categories_series.str.join('').apply(lambda x: x[1:-1])\n",
    "\n",
    "# business_id, categories table\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "categories_mat = vectorizer.fit_transform(categories_series).toarray()\n",
    "categories = vectorizer.get_feature_names()\n",
    "df_categories = pd.DataFrame(categories_mat,\n",
    "                             columns=categories, \n",
    "                             index=categories_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acai</th>\n",
       "      <th>acarons</th>\n",
       "      <th>accessories</th>\n",
       "      <th>acos</th>\n",
       "      <th>active</th>\n",
       "      <th>activities</th>\n",
       "      <th>acupuncture</th>\n",
       "      <th>adoption</th>\n",
       "      <th>adult</th>\n",
       "      <th>afes</th>\n",
       "      <th>...</th>\n",
       "      <th>wraps</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yogur</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>zakaya</th>\n",
       "      <th>zechuan</th>\n",
       "      <th>zoos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--9e1ONYQuAa-CB_Rrw7Tw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0BxAGlIk5DJAGVkpqBXxg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0RkJ_uIduNLWQrphbADRw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        acai  acarons  accessories  acos  active  activities  \\\n",
       "business_id                                                                    \n",
       "--9e1ONYQuAa-CB_Rrw7Tw     0        0            0     0       0           0   \n",
       "-0BxAGlIk5DJAGVkpqBXxg     0        0            0     0       0           0   \n",
       "-0RkJ_uIduNLWQrphbADRw     0        0            0     0       0           0   \n",
       "\n",
       "                        acupuncture  adoption  adult  afes  ...  wraps  yelp  \\\n",
       "business_id                                                 ...                \n",
       "--9e1ONYQuAa-CB_Rrw7Tw            0         0      0     0  ...      0     0   \n",
       "-0BxAGlIk5DJAGVkpqBXxg            0         0      0     0  ...      0     0   \n",
       "-0RkJ_uIduNLWQrphbADRw            0         0      0     0  ...      0     0   \n",
       "\n",
       "                        yoga  yogur  yogurt  your  yourself  zakaya  zechuan  \\\n",
       "business_id                                                                    \n",
       "--9e1ONYQuAa-CB_Rrw7Tw     0      0       0     0         0       0        0   \n",
       "-0BxAGlIk5DJAGVkpqBXxg     0      0       0     0         0       0        0   \n",
       "-0RkJ_uIduNLWQrphbADRw     0      0       0     0         0       0        0   \n",
       "\n",
       "                        zoos  \n",
       "business_id                   \n",
       "--9e1ONYQuAa-CB_Rrw7Tw     0  \n",
       "-0BxAGlIk5DJAGVkpqBXxg     0  \n",
       "-0RkJ_uIduNLWQrphbADRw     0  \n",
       "\n",
       "[3 rows x 861 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categories.head(3) # 861 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only choose 150 features out of 861. We use `TruncatedSVD` to implement dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use svd to reduce dimension\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# initialize a SVD\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "# fit SVD on categories_mat\n",
    "svd.fit(categories_mat)\n",
    "# transform original data\n",
    "categories_svd = svd.transform(categories_mat)\n",
    "# create a new dataframe\n",
    "df_categories_svd = pd.DataFrame(categories_svd,\n",
    "                                 index=categories_series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might check for the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total variance explained 0.8999518166452347\n",
      "df_average :  (4249, 5)\n",
      "df_categories :  (4249, 861)\n",
      "df_categories_svd :  (4249, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"total variance explained\", svd.explained_variance_ratio_.sum()) \n",
    "\n",
    "# display feature matrix dimension\n",
    "print ('df_average : ', df_average.shape)\n",
    "print ('df_categories : ', df_categories.shape)\n",
    "print ('df_categories_svd : ', df_categories_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df_average and df_categories_svd\n",
    "df_business = df_average.join(df_categories_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible way to recommend a restaurant to a user is to compute the similarity matrix between the restaurants he has visited and new restaurants. The similarity score is based on features we have got before. Another way, which we will implement today, is to build a predictive model. Let's say the restaurants whose average rating scores are larger than or equal to 4.0 will be recommended. \n",
    "\n",
    "This is the classification problem. We choose Random Forest Classifier as our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get X and y\n",
    "target = 'stars'\n",
    "features = [x for x in df_business.columns if x not in ['avg_stars','stars']]\n",
    "y = df_business[target].values >= 4.0\n",
    "X = df_business[features].values\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.6624 (+/- 0.0073) [lr]\n",
      "Accuracy: 0.6005 (+/- 0.0142) [dt]\n",
      "Accuracy: 0.6214 (+/- 0.0132) [rf]\n",
      "Accuracy: 0.6547 (+/- 0.0020) [ada]\n",
      "Accuracy: 0.6668 (+/- 0.0095) [xgb]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# candidate models\n",
    "models = [LogisticRegression(), \n",
    "          DecisionTreeClassifier(), \n",
    "          RandomForestClassifier(),\n",
    "          AdaBoostClassifier(),\n",
    "          XGBClassifier()]\n",
    "\n",
    "labels = ['lr','dt','rf','ada','xgb']\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "# train models\n",
    "for model, label in zip(models,labels):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is not that good, with 66% accuracy score for Logistic Regression and XGBoost. For one reason, the sample size is small, compared with number of features. You might choose more dataset in **Lecture5.3_Preprocessing**, if you are interested in improving the accuracy performance here. But let's focus more on the Collaborative Filtering Recommender in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering tries to predict the rating or preference that a user would give an item-based on past ratings and preferences of other users. It does not require item metadata like its content-based counterparts.\n",
    "\n",
    "Earlier collaborative filtering systems based on rating similarity between users (known as user-user collaborative filtering) had several problems:\n",
    "\n",
    "* systems performed poorly when they had many items but comparatively few ratings\n",
    "* computing similarities between all pairs of users was expensive\n",
    "* user profiles changed quickly and the entire system model had to be recomputed\n",
    "\n",
    "Item-item models resolve these problems in systems that have more users than items. Therefore, when we talk about Collaborative Filtering, usually we mean item-item CF.\n",
    "\n",
    "There are three types of [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n",
    "\n",
    "* [Memory-based](https://en.wikipedia.org/wiki/Collaborative_filtering#Memory-based): compute the similarity matrix and find nearest neighbors.\n",
    "* [Model-based](https://en.wikipedia.org/wiki/Collaborative_filtering#Model-based): singular value decomposition or matrix factorization.\n",
    "* [Hybrid](https://en.wikipedia.org/wiki/Collaborative_filtering#Hybrid)\n",
    "\n",
    "Let's implement Memory-based by hand and Model-based by `surprise`. In the end, we give a brief introduction to Hybrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Memory-based Recommender, we need to find the similarity matrix. Then we compute nearest neighborhood weighted average to make prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, let's take a look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of ratings: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique business_id (restaurant): \n",
      "4249\n",
      "\n",
      "Number of unique user_id (user)\n",
      "102800\n"
     ]
    }
   ],
   "source": [
    "# Get business_id, user_id, stars for recommender\n",
    "df_stars = df[['business_id', 'user_id', 'stars']]\n",
    "\n",
    "print (\"Distribution of ratings: \")\n",
    "plt.bar(df_stars['stars'].value_counts().index, df_stars['stars'].value_counts().values)\n",
    "plt.show()\n",
    "print ()\n",
    "\n",
    "print (\"Number of unique business_id (restaurant): \")\n",
    "print (df_stars['business_id'].nunique())\n",
    "print ()\n",
    "\n",
    "print (\"Number of unique user_id (user)\")\n",
    "print (df_stars['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might easily find that many users give only 1 or 2 reviews. We should exclude these users from our data, since it has little information as well as reduces the efficiency of algorithm. \n",
    "\n",
    "Let's assume an active user is the one who comments over 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71200 users only comment once.\n",
      "After filtering, we only have 4133 users.\n",
      "Total number of comments is: 44701\n"
     ]
    }
   ],
   "source": [
    "df_user_counts = df_stars['user_id'].value_counts()\n",
    "\n",
    "print (\"%d users only comment once.\" %df_user_counts[df_user_counts == 1].sum())\n",
    "\n",
    "df_users = df_user_counts[df_user_counts > 5]\n",
    "\n",
    "print (\"After filtering, we only have %d users.\" %df_users.count())\n",
    "print (\"Total number of comments is: %d\"%df_users.sum())\n",
    "\n",
    "# filter the rating table\n",
    "df_stars_cleaned = df_stars.set_index('user_id').loc[df_users.index,:].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a similarity matrix, first we need to generate an utility matrix, in which the value is rating score, index is user id and column is business id. We can use `pd.pivot_table` to implement this transformation.\n",
    "\n",
    "Then, we use `sklearn.metrics.pairwise.cosine_similarity` to compute similarity matrix. Please note that this is item-item collaborative filtering, thus dimension of similarity matrix should be equal to number of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# convert to document, word like matrix\n",
    "df_utility = pd.pivot_table(data=df_stars_cleaned,\n",
    "                            values='stars', # fill with stars\n",
    "                            index='user_id', # rows\n",
    "                            columns='business_id', # columns\n",
    "                            fill_value=0) # fill missings\n",
    "\n",
    "# Item-Item Similarity Matrix\n",
    "item_sim_mat = cosine_similarity(df_utility.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for similarity dimension\n",
    "assert item_sim_mat.shape[0] == df_stars_cleaned['business_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make prediction for a user, we need to find nearest neighborhood in order to compute weighted average score. Let's first find the indexes of nearest neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort to get the index of sorted similarity\n",
    "least_to_most_sim_indexes = np.argsort(item_sim_mat, axis=1)\n",
    "\n",
    "# number of neighborhoods (hyperparameters)\n",
    "neighborhood_size = 75\n",
    "neighborhoods = least_to_most_sim_indexes[:, -neighborhood_size-1:-1] # remove the last one, since it is itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make prediction, give a user. Let's pick a lucky user `'qKUEhJUl0Z9MFW7R7BTvRQ'`. \n",
    "\n",
    "* First, we need to find the items that are rated by this user. \n",
    "* For each item $i$ to be rated, \n",
    "    * Find the nearest neighbors of this item $N_i$, then intersect with the items rated by this user $I_u$.\n",
    "    * The predicted rating is the weighter average, $$rating(u,i) = \\frac{\\sum_{j\\in N_i \\cap I_u} similarity(i,j) * r_{u,j}}{\\sum_{j\\in N_i \\cap I_u} similarity(i,j)}$$\n",
    "    * $similarity(i,j)$ is the similarity between item $i$ and $j$, $r_{u,j}$ is the rating user $u$ gives item $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4606816 1.        1.        ... 5.        5.        1.       ]\n"
     ]
    }
   ],
   "source": [
    "# pick a lucky user\n",
    "user_id = 'qKUEhJUl0Z9MFW7R7BTvRQ'\n",
    "\n",
    "n_users = df_utility.shape[0]\n",
    "n_items = df_utility.shape[1]\n",
    "\n",
    "items_rated_by_this_user = df_utility.loc[user_id,:].values.nonzero()[0]\n",
    "# Just initializing so we have somewhere to put rating preds\n",
    "out = np.zeros(n_items)\n",
    "for item_to_rate in range(n_items):\n",
    "    relevant_items_idx = np.intersect1d(neighborhoods[item_to_rate],\n",
    "                                    items_rated_by_this_user,\n",
    "                                    assume_unique=True)  # assume_unique speeds up intersection op\n",
    "    relevant_items = df_utility.columns[relevant_items_idx].values\n",
    "    out[item_to_rate] = df_utility.loc[user_id, relevant_items].values.dot(\n",
    "        item_sim_mat[item_to_rate, relevant_items_idx].T) / item_sim_mat[item_to_rate, relevant_items_idx].sum()\n",
    "\n",
    "pred_ratings = np.nan_to_num(out)\n",
    "print (pred_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our final recommendation, based on the predicted rating in the previous section. Top 10 restaurants are recommended in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PM0A7ZgbxvtbRCj931PbyQ', 'h4Q-uSoOIhMoIsOhat6l3A',\n",
       "       'J1MVP1oGbBnKVqriX4HDuQ', '0_VT3sTwi7gorIlU36ASmg',\n",
       "       'jSu2A7DibDjmUd3wUt6fPg', '0Myf2fOlXXNHgIuPZoOlsA',\n",
       "       'oE-vW0S6wRh0YVki7_pVBA', '4yJVSK-Q5UkED6RjORRssA',\n",
       "       'cjlYcqxyqRBe_w3OgasPAw', 'XtE_J8DZXgjFbW1xVkqmUw'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter: Recommend n movies\n",
    "n = 10\n",
    "\n",
    "# Get item indexes sorted by predicted rating\n",
    "item_index_sorted_by_pred_rating = list(np.argsort(pred_ratings))\n",
    "\n",
    "# Find items that have been rated by user\n",
    "item_index_rated_by_this_user = df_utility.loc[user_id,:].values.nonzero()[0]\n",
    "\n",
    "# We want to exclude the items that have been rated by user\n",
    "unrated_items_by_pred_rating = [item for item in item_index_sorted_by_pred_rating\n",
    "                                if item not in item_index_rated_by_this_user]\n",
    "\n",
    "df_utility.columns[unrated_items_by_pred_rating[-n:]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our recommender system, we should compare the predicted rating to the true rating in our dataset. Let's only evaluate this single user, though in reality all user should be predicted and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1.5547033931153946\n",
      "MAE:  1.261639772554199\n"
     ]
    }
   ],
   "source": [
    "# get item id that is rated by this user\n",
    "item_rated_by_this_user = df_utility.columns[item_index_rated_by_this_user].values\n",
    "\n",
    "# prediction rating score\n",
    "y_pred = pred_ratings[item_index_rated_by_this_user]\n",
    "\n",
    "# true rating score\n",
    "y_true = df_utility.loc[user_id, item_rated_by_this_user].values\n",
    "\n",
    "# RMSE\n",
    "print (\"RMSE: \", np.sqrt(np.mean((y_pred - y_true)**2)))\n",
    "\n",
    "# MAE\n",
    "print (\"MAE: \", np.mean(np.abs(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RMSE of 1.55 and MAE of 1.26 are derived for this user. You might try other users by changing the `user_id`. The result is fine, but not that good. The high mean absolute error indicates that a predicted rating of say 4.5, may roughly vary between 3.14 to 5.0. This may lead to the true situation of not recommendation, but we actually recommend this item (4.5>4). \n",
    "\n",
    "Tuning hyperparameter `number of neighborhood` may improve the performance, but let's try model-based recommender system using `surprise` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Surprise](https://surprise.readthedocs.io/en/stable/index.html) is an easy-to-use Python scikit for recommender systems. It has a lot of builtin datasets that you can play with. Alternatively, you can of course use a custom dataset as well. You should load a rating matrix from csv file or from a pandas dataframe. Either way, you will need to define a `Reader` object for `Surprise` to be able to parse the file or dataframe.\n",
    "\n",
    "To load a dataset from a pandas dataframe, you will need the `load_from_df()` method. You will also need a `Reader` object, but only the `rating_scale` parameter must be specified. The dataframe must have three columns, corresponding to the **user (raw) ids**, the **item (raw) ids**, and **the ratings** in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_stars[['user_id', 'business_id', 'stars']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn the dataframe into `surprise.Dataset`. Let's apply SVD, NMF, SVDpp, KNNBasic to our dataset. To better visualize the results, we create a dictionary that maps algorithm names to reference links. We will also be using cross validation to train model and get average RMSE and MAE on validation set. It may run up to 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,unicode_literals)\n",
    "\n",
    "from surprise import KNNBasic, SVD, SVDpp, NMF\n",
    "from surprise.model_selection import KFold\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import datetime\n",
    "import six\n",
    "\n",
    "classes = (SVD, SVDpp, NMF)\n",
    "# dict to map algo names and datasets to their markdown links in the table\n",
    "stable = 'http://surprise.readthedocs.io/en/stable/'\n",
    "LINK = {'SVD': '[{}]({})'.format('SVD',\n",
    "                                 stable +\n",
    "                                 'matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD'),\n",
    "        'SVDpp': '[{}]({})'.format('SVD++',\n",
    "                                   stable +\n",
    "                                   'matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp'),\n",
    "        'NMF': '[{}]({})'.format('NMF',\n",
    "                                 stable +\n",
    "                                 'matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF'),\n",
    "        'Yelp Dataset': '[{}]({})'.format('Yelp Dataset',\n",
    "                                     'https://www.yelp.com/dataset')\n",
    "        }\n",
    "# set seed\n",
    "np.random.seed(42)\n",
    "# folds will be the same for all algorithms.\n",
    "kf = KFold(random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [Yelp Dataset](https://www.yelp.com/dataset)                                                                                          |   RMSE |   MAE | Time    |\n",
      "|:--------------------------------------------------------------------------------------------------------------------------------------|-------:|------:|:--------|\n",
      "| [SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)     |  1.314 | 1.057 | 0:00:40 |\n",
      "| [SVD++](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp) |  1.339 | 1.083 | 0:01:44 |\n",
      "| [NMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)     |  1.527 | 1.23  | 0:01:09 |\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "for klass in classes:\n",
    "    start = time.time()\n",
    "    out = cross_validate(klass(), data, ['rmse', 'mae'], kf)\n",
    "    cv_time = str(datetime.timedelta(seconds=int(time.time() - start)))\n",
    "    link = LINK[klass.__name__]\n",
    "    mean_rmse = '{:.3f}'.format(np.mean(out['test_rmse']))\n",
    "    mean_mae = '{:.3f}'.format(np.mean(out['test_mae']))\n",
    "\n",
    "    new_line = [link, mean_rmse, mean_mae, cv_time]\n",
    "#     print(tabulate([new_line], tablefmt=\"pipe\"))\n",
    "    table.append(new_line)\n",
    "\n",
    "header = [LINK['Yelp Dataset'],'RMSE','MAE','Time']\n",
    "print(tabulate(table, header, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [Yelp Dataset](https://www.yelp.com/dataset)                                                                                          |   RMSE |   MAE | Time    |\n",
    "|:--------------------------------------------------------------------------------------------------------------------------------------|-------:|------:|:--------|\n",
    "| [SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)     |  1.314 | 1.057 | 0:00:40 |\n",
    "| [SVD++](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp) |  1.339 | 1.083 | 0:01:44 |\n",
    "| [NMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)     |  1.527 | 1.23  | 0:01:09 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, SVD gets best result, with 1.314 RMSE and 1.057 MAE. In this situation, model-based CF is better than Memory-based CF. However, both of them suffer two major challenges: data sparsity and scalability. A hybrid method combining memory-based and model-based can overcome these problems. We will give a brief introduction of Hybrid Recommender in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Hybrid](https://en.wikipedia.org/wiki/Collaborative_filtering#Hybrid) method overcome the limitations of traditional CF approaches and improve prediction performance. However, they have increased complexity and are expensive to implement. \n",
    "\n",
    "In [Hybrid User-Item Based Collaborative Filtering](https://reader.elsevier.com/reader/sd/pii/S1877050915023492?token=494AE625B6CDBE7BCCC5E9D2AA0C20D95BED53871AF0211228E6EA4E9609E5549FDE39ED618C34E7F2F3ADDBE6FE4FBA), the authors proposed a hybrid method based on item-item CF. \n",
    "* Case Based Reasoning with averaging filing is used to handle sparsity problem. \n",
    "* Self-Organizing Map (SOM) optimized with Genetic Algorithm (GA) performs user clustering in large datasets to reduce the scope. \n",
    "\n",
    "In 2007, Google also gives a scalable online CF method, in order to solve large and dynamic settings problem. In [Google News Personalization: Scalable Online Collaborative Filtering](https://www2007.org/papers/paper570.pdf), they generate recommendations for google news by three approaches.\n",
    "* collaborative filtering using MinHash clustering (Jaccard coefficient)\n",
    "* Probabilistic Latent Semantic Indexing (PLSI): The relationship between users and items is learned by modeling the joint distribution of users and items as a mixture distribution\n",
    "* covisitation counts: covisitation is defined as an event in which two stories are clicked by the same user within a certain time interval (typically set to a few hours).\n",
    "\n",
    "Then they combine recommendations from different algorithms using a linear model (like stacking). \n",
    "\n",
    "The recommender system is very personalized for different companies with different products. For example, in the content of Google News Recommender, the rating is binary. A click on a story corresponds to a 1 rating, while a non-click corresponds to a 0 rating. In the content of movie or restaurant recommender, however, the rating scale is 1 to 5. \n",
    "\n",
    "In addition, this article [Collaboration via Content](http://recommender-systems.org/hybrid-recommender-systems) compared the collaboration with content. A hybrid method can also be the combination of content-based and CF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
